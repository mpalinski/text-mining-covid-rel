{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "84163f3ca19c0b7c9fda47121b3bc4cadfaf1fcc"
   },
   "source": [
    "# Gensim Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d96105f0c90bf052b2afdb684bf31549e1e6c81"
   },
   "source": [
    "# Briefing about Word2Vec:\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" alt=\"drawing\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "cc7b3e6ca62670ff13626705402f626778487204"
   },
   "outputs": [],
   "source": [
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michalpalinski/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/michalpalinski/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/michalpalinski/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0c36323d9aa62f74ab348cda5ee0f571aa1d4a96"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "6453b9c3f797e51923e030090ead659253f4e459"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(819, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/clean/sec_pol_clean.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>f_name</th>\n",
       "      <th>cleaned_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/sec_POL/Fakt/Factiva-20230808-125</td>\n",
       "      <td>Ksiądz Janusz Koplewski l Odwagi Wirus to nie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/sec_POL/Rzeczpospolita/Factiva-2023080...</td>\n",
       "      <td>Kraj bez Boga Wörter Januar    © Copyright  Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>../data/sec_POL/Rzeczpospolita/Factiva-2023080...</td>\n",
       "      <td>Jaka Polska po rządach PiS Wörter Januar    © ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>../data/sec_POL/Rzeczpospolita/Factiva-2023080...</td>\n",
       "      <td>Watykan Rosja Chiny W co naprawdę gra Francisz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>../data/sec_POL/Rzeczpospolita/Factiva-2023080...</td>\n",
       "      <td>Górski Cieślik Dokąd jechać dokąd nie i dlacze...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             f_name  \\\n",
       "0           0          ../data/sec_POL/Fakt/Factiva-20230808-125   \n",
       "1           1  ../data/sec_POL/Rzeczpospolita/Factiva-2023080...   \n",
       "2           2  ../data/sec_POL/Rzeczpospolita/Factiva-2023080...   \n",
       "3           3  ../data/sec_POL/Rzeczpospolita/Factiva-2023080...   \n",
       "4           4  ../data/sec_POL/Rzeczpospolita/Factiva-2023080...   \n",
       "\n",
       "                                         cleaned_txt  \n",
       "0  Ksiądz Janusz Koplewski l Odwagi Wirus to nie ...  \n",
       "1  Kraj bez Boga Wörter Januar    © Copyright  Al...  \n",
       "2  Jaka Polska po rządach PiS Wörter Januar    © ...  \n",
       "3  Watykan Rosja Chiny W co naprawdę gra Francisz...  \n",
       "4  Górski Cieślik Dokąd jechać dokąd nie i dlacze...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "82cb38f176526679f66ee31e11cfe4f5eebdab51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "f_name         0\n",
       "cleaned_txt    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna().reset_index(drop=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['title']=df['title'].fillna('')\n",
    "df['text_all']=df['cleaned_txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f07dca2a2656dcd9e0c315afa36af32a992eef7"
   },
   "source": [
    "## Cleaning:\n",
    "We are lemmatizing and removing the stopwords and non-alphabetic characters for each line of dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pl-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pl_core_news_sm-3.7.0/pl_core_news_sm-3.7.0-py3-none-any.whl (20.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from pl-core-news-sm==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (1.10.13)\n",
      "Requirement already satisfied: jinja2 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (1.26.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/michalpalinski/anaconda3/envs/llm/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->pl-core-news-sm==3.7.0) (2.1.3)\n",
      "Installing collected packages: pl-core-news-sm\n",
      "Successfully installed pl-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pl_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# %%capture \n",
    "!python -m spacy download pl_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "b26a0c01c5701630d3951cfc808a9d944eea6371"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('pl_core_news_sm', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 5:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f686964722eede40e5961cd232aee7b6dd587bd1"
   },
   "source": [
    "Removes non-alphabetic characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "b45598934171607242ca7d50f8c5f7c91411aace"
   },
   "outputs": [],
   "source": [
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['text_all'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2360160a7f326a32d56f2f18782d7ce2f4ac1def"
   },
   "source": [
    "Taking advantage of spaCy .pipe() attribute to speed-up the cleaning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa44ca458c970ca229426779e6ffcd46c2de313c"
   },
   "outputs": [],
   "source": [
    "# t = time()\n",
    "\n",
    "# txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1, cleanup=True)]\n",
    "\n",
    "# print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(document):\n",
    "        # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        tokens = document.split()\n",
    "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in en_stop]\n",
    "        tokens = [word for word in tokens if len(word) > 3]\n",
    "\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "        return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~df['text_all'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=df['text_all'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus = [preprocess_text(sentence) for sentence in docs if sentence.strip() !='']\n",
    "\n",
    "word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n",
    "word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean']=final_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean=df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65ec76c60f9fe93bba6909f4e696f90e3e54710b"
   },
   "source": [
    "Put the results in a DataFrame to remove missing values and duplicates:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "31b4a744059df490ddb47ab6cdec008dc929ede3"
   },
   "source": [
    "## Bigrams:\n",
    "We are using Gensim Phrases package to automatically detect common phrases (bigrams) from a list of sentences.\n",
    "https://radimrehurek.com/gensim/models/phrases.html\n",
    "\n",
    "The main reason we do this is to catch words like \"mr_burns\" or \"bart_simpson\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "af6d420284a0ff7a7407d4c526754ffe850d6170"
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "788aec3c82788101db25d4ca6105ee133fecae7c"
   },
   "source": [
    "As `Phrases()` takes a list of list of words as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "f58487ff08d8812622fd7aef36139f1c850add18"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:36:29: collecting all words and their counts\n",
      "INFO - 16:36:29: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 16:36:30: collected 676918 token types (unigram + bigrams) from a corpus of 771399 words and 819 sentences\n",
      "INFO - 16:36:30: merged Phrases<676918 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 16:36:30: Phrases lifecycle event {'msg': 'built Phrases<676918 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 1.25s', 'datetime': '2024-01-18T16:36:30.312982', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:19:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO - 16:36:30: exporting phrases from Phrases<676918 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 16:36:31: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<199 phrases, min_count=30, threshold=10.0> from Phrases<676918 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 1.26s', 'datetime': '2024-01-18T16:36:31.575717', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:19:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "sent = [row.split() for row in df_clean['clean']]\n",
    "phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb7766b322cbc1d3381912b890585eb249ac5304"
   },
   "source": [
    "Creates the relevant phrases from the list of sentences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4f81e8bb2c09a67b00cd24db28353eca8ae188c"
   },
   "source": [
    "## Most Frequent Words:\n",
    "Mainly a sanity check of the effectiveness of the lemmatization, removal of stopwords, and addition of bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "eeb8afe1cfcb7ba65bd14d657455600acacf39ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96658"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "5b010149150b2b2eaf332d79bcde0649b8a3c2b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jest',\n",
       " 'przez',\n",
       " 'tego',\n",
       " 'tylko',\n",
       " 'może',\n",
       " 'będzie',\n",
       " 'które',\n",
       " 'który',\n",
       " 'roku',\n",
       " 'jego']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "500ab7b5c84dc006d7945f339c40725a82856fdf"
   },
   "source": [
    "# Training the model\n",
    "## Gensim Word2Vec Implementation:\n",
    "We use Gensim implementation of word2vec: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "3269be205cadbad499aa87890893d92da6adc796"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c524bc49c41a6c37f9e754a38797c9501202090"
   },
   "source": [
    "## Why I seperate the training of the model in 3 steps:\n",
    "I prefer to separate the training in 3 distinctive steps for clarity and monitoring.\n",
    "1. `Word2Vec()`: \n",
    ">In this first step, I set up the parameters of the model one-by-one. <br>I do not supply the parameter `sentences`, and therefore leave the model uninitialized, purposefully.\n",
    "2. `.build_vocab()`: \n",
    ">Here it builds the vocabulary from a sequence of sentences and thus initialized the model. <br>With the loggings, I can follow the progress and even more important, the effect of `min_count` and `sample` on the word corpus. I noticed that these two parameters, and in particular `sample`, have a great influence over the performance of a model. Displaying both allows for a more accurate and an easier management of their influence.\n",
    "3. `.train()`:\n",
    ">Finally, trains the model.<br>\n",
    "The loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "03488d9b68963579c96094aca88a302c9f2753a7"
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89c305fcd163488441ac2ac6133678bd973b4419"
   },
   "source": [
    "## The parameters:\n",
    "\n",
    "* `min_count` <font color='purple'>=</font> <font color='green'>int</font> - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "\n",
    "\n",
    "* `window` <font color='purple'>=</font> <font color='green'>int</font> - The maximum distance between the current and predicted word within a sentence. E.g. `window` words on the left and `window` words on the left of our target - (2, 10)\n",
    "\n",
    "\n",
    "* `size` <font color='purple'>=</font> <font color='green'>int</font> - Dimensionality of the feature vectors. - (50, 300)\n",
    "\n",
    "\n",
    "* `sample` <font color='purple'>=</font> <font color='green'>float</font> - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial.  - (0, 1e-5)\n",
    "\n",
    "\n",
    "* `alpha` <font color='purple'>=</font> <font color='green'>float</font> - The initial learning rate - (0.01, 0.05)\n",
    "\n",
    "\n",
    "* `min_alpha` <font color='purple'>=</font> <font color='green'>float</font> - Learning rate will linearly drop to `min_alpha` as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "\n",
    "\n",
    "* `negative` <font color='purple'>=</font> <font color='green'>int</font> - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "\n",
    "\n",
    "* `workers` <font color='purple'>=</font> <font color='green'>int</font> - Use these many worker threads to train the model (=faster training with multicore machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "ad619db82c219d6cb81fad516563feb0c4d474cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:36:44: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.03>', 'datetime': '2024-01-18T16:36:44.818611', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:19:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=5,\n",
    "                     # size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7e9f1bd338f9e15647b5209ffd8fbb131cd7ee5"
   },
   "source": [
    "## Building the Vocabulary Table:\n",
    "Word2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "66358ad743e05e17dfbed3899af9c41056143daa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:36:46: collecting all words and their counts\n",
      "INFO - 16:36:46: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 16:36:47: collected 96658 word types from a corpus of 757989 raw words and 819 sentences\n",
      "INFO - 16:36:47: Creating a fresh vocabulary\n",
      "INFO - 16:36:47: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 6002 unique words (6.21% of original 96658, drops 90656)', 'datetime': '2024-01-18T16:36:47.757087', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:19:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 16:36:47: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 487189 word corpus (64.27% of original 757989, drops 270800)', 'datetime': '2024-01-18T16:36:47.757758', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:19:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 16:36:47: deleting the raw counts dictionary of 96658 items\n",
      "INFO - 16:36:47: sample=6e-05 downsamples 1380 most-common words\n",
      "INFO - 16:36:47: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 309809.70900083485 word corpus (63.6%% of prior 487189)', 'datetime': '2024-01-18T16:36:47.795543', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:19:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 16:36:47: estimated required memory for 6002 words and 100 dimensions: 7802600 bytes\n",
      "INFO - 16:36:47: resetting layer weights\n",
      "INFO - 16:36:47: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-01-18T16:36:47.865816', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:19:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63260d82061abb47db7f2f8b23e07ec629adf5a9"
   },
   "source": [
    "## Training of the model:\n",
    "_Parameters of the training:_\n",
    "* `total_examples` <font color='purple'>=</font> <font color='green'>int</font> - Count of sentences;\n",
    "* `epochs` <font color='purple'>=</font> <font color='green'>int</font> - Number of iterations (epochs) over the corpus - [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "07a2a047e701e512fd758edff186daadaeea6461"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:36:48: Word2Vec lifecycle event {'msg': 'training model with 3 workers on 6002 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=5 shrink_windows=True', 'datetime': '2024-01-18T16:36:48.778136', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:19:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "INFO - 16:36:49: EPOCH 0: training on 757989 raw words (309692 effective words) took 1.0s, 311200 effective words/s\n",
      "INFO - 16:36:50: EPOCH 1: training on 757989 raw words (310156 effective words) took 1.0s, 311681 effective words/s\n",
      "INFO - 16:36:51: EPOCH 2 - PROGRESS: at 79.85% examples, 252956 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 16:36:51: EPOCH 2: training on 757989 raw words (309683 effective words) took 1.1s, 269526 effective words/s\n",
      "INFO - 16:36:52: EPOCH 3 - PROGRESS: at 90.84% examples, 284811 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:36:53: EPOCH 3: training on 757989 raw words (309401 effective words) took 1.1s, 287029 effective words/s\n",
      "INFO - 16:36:54: EPOCH 4 - PROGRESS: at 56.04% examples, 188954 words/s, in_qsize 5, out_qsize 2\n",
      "INFO - 16:36:54: EPOCH 4: training on 757989 raw words (310324 effective words) took 1.3s, 235464 effective words/s\n",
      "INFO - 16:36:55: EPOCH 5 - PROGRESS: at 94.75% examples, 293831 words/s, in_qsize 4, out_qsize 0\n",
      "INFO - 16:36:55: EPOCH 5: training on 757989 raw words (309601 effective words) took 1.0s, 298923 effective words/s\n",
      "INFO - 16:36:56: EPOCH 6 - PROGRESS: at 71.67% examples, 233933 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 16:36:56: EPOCH 6: training on 757989 raw words (310265 effective words) took 1.2s, 249154 effective words/s\n",
      "INFO - 16:36:57: EPOCH 7 - PROGRESS: at 81.44% examples, 261139 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:36:57: EPOCH 7: training on 757989 raw words (309572 effective words) took 1.1s, 278357 effective words/s\n",
      "INFO - 16:36:58: EPOCH 8 - PROGRESS: at 56.04% examples, 190473 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:36:59: EPOCH 8: training on 757989 raw words (310060 effective words) took 1.4s, 219827 effective words/s\n",
      "INFO - 16:37:00: EPOCH 9 - PROGRESS: at 66.79% examples, 217622 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:37:00: EPOCH 9: training on 757989 raw words (309880 effective words) took 1.3s, 235313 effective words/s\n",
      "INFO - 16:37:01: EPOCH 10 - PROGRESS: at 67.64% examples, 221095 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 16:37:02: EPOCH 10: training on 757989 raw words (309959 effective words) took 1.4s, 214487 effective words/s\n",
      "INFO - 16:37:03: EPOCH 11 - PROGRESS: at 49.33% examples, 164115 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:37:03: EPOCH 11: training on 757989 raw words (309937 effective words) took 1.7s, 178032 effective words/s\n",
      "INFO - 16:37:04: EPOCH 12 - PROGRESS: at 77.17% examples, 244699 words/s, in_qsize 5, out_qsize 1\n",
      "INFO - 16:37:04: EPOCH 12: training on 757989 raw words (309920 effective words) took 1.2s, 266955 effective words/s\n",
      "INFO - 16:37:05: EPOCH 13 - PROGRESS: at 79.85% examples, 256921 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 16:37:06: EPOCH 13: training on 757989 raw words (309819 effective words) took 1.1s, 276071 effective words/s\n",
      "INFO - 16:37:07: EPOCH 14 - PROGRESS: at 78.39% examples, 249676 words/s, in_qsize 3, out_qsize 1\n",
      "INFO - 16:37:07: EPOCH 14: training on 757989 raw words (309468 effective words) took 1.2s, 261362 effective words/s\n",
      "INFO - 16:37:07: Word2Vec lifecycle event {'msg': 'training on 11369835 raw words (4647737 effective words) took 18.5s, 251215 effective words/s', 'datetime': '2024-01-18T16:37:07.281392', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:19:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.31 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=15, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48e12768512b82c2d5cf6a543e3a9f2515699a22"
   },
   "source": [
    "As we do not plan to train the model any further, we are calling init_sims(), which will make the model much more memory-efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "34dd51c7f2f39d016b982ef81e4df576f6b31bcb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7q/c3c06q9s7m552ts_r68zhmz00000gn/T/ipykernel_8248/514372312.py:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v_model.init_sims(replace=True)\n",
      "WARNING - 16:37:07: destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    }
   ],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a420d5a98eb860cff1f4bbac8cbe2054459b6200"
   },
   "source": [
    "# Exploring the model\n",
    "## Most similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "339207a733a1ac42fe60e32a29f9e5d5ca0a9275"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"epidemia\"],topn=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "773c0acc8750ba8e728ff261f2e9ec39694c245c"
   },
   "source": [
    "### t-SNE visualizations:\n",
    "t-SNE is a non-linear dimensionality reduction algorithm that attempts to represent high-dimensional data and the underlying relationships between vectors in a lower-dimensional space.<br>\n",
    "Here is a good tutorial on it: https://medium.com/@luckylwk/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27ec46110042fc28da900b1b344ae4e0692d5dc2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22693eaa25253b38cee3c5cd5db6b6fdddb575a4"
   },
   "source": [
    "Our goal in this section is to plot our 300 dimensions vectors into 2 dimensional graphs, and see if we can spot interesting patterns.<br>\n",
    "For that we are going to use t-SNE implementation from scikit-learn.\n",
    "\n",
    "To make the visualizations more relevant, we will look at the relationships between a query word (in <font color='red'>**red**</font>), its most similar words in the model (in <font color=\"blue\">**blue**</font>), and other words from the vocabulary (in <font color='green'>**green**</font>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "489a7d160dcd92da0ce42a3b5b461368c9ffe5f1"
   },
   "outputs": [],
   "source": [
    "def tsnescatterplot(model, word, list_names):\n",
    "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
    "    its list of most similar words, and a list of words.\n",
    "    \"\"\"\n",
    "    arrays = np.empty((0, 300), dtype='f')\n",
    "    word_labels = [word]\n",
    "    color_list  = ['red']\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "    \n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.__getitem__([wrd])\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "        \n",
    "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
    "    reduc = PCA(n_components=50).fit_transform(arrays)\n",
    "    \n",
    "    # Finds t-SNE coordinates for 2 dimensions\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
    "                       'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels,\n",
    "                       'color': color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={'s': 40,\n",
    "                                  'facecolors': df['color']\n",
    "                                 }\n",
    "                    )\n",
    "    \n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df['y'][line],\n",
    "                 '  ' + df[\"words\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 color=df['color'][line],\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 62643,
     "sourceId": 120953,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2696764,
     "sourceId": 4636238,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3657307,
     "sourceId": 6350747,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3665903,
     "sourceId": 6363589,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3666104,
     "sourceId": 6363860,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3666134,
     "sourceId": 6363895,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3946671,
     "sourceId": 6867149,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4322343,
     "sourceId": 7427983,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4322832,
     "sourceId": 7428667,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4323001,
     "sourceId": 7428903,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 12784,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
